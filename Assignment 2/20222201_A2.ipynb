{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0eff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading & Preprocessing (Spark)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, least, when, collect_set, explode, size, min as spark_min, count, array_intersect, avg, sum as spark_sum\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Assignment2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read as DataFrame (skip commented lines starting with #)\n",
    "edges = (\n",
    "    spark.read.option(\"comment\", \"#\")\n",
    "    .option(\"delimiter\", \"\\t\")\n",
    "    .csv(\"../Wiki-Vote.txt\")\n",
    "    .select(\n",
    "        col(\"_c0\").cast(\"int\").alias(\"src\"),\n",
    "        col(\"_c1\").cast(\"int\").alias(\"dst\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get unique vertices\n",
    "vertices = edges.select(\"src\").union(edges.select(\"dst\")).distinct().withColumnRenamed(\"src\", \"id\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8718405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in graph: 7115\n",
      "Number of edges in graph: 103689\n"
     ]
    }
   ],
   "source": [
    "# 2. Basic Graph Statistics\n",
    "num_nodes = vertices.count()\n",
    "num_edges = edges.count()\n",
    "\n",
    "print(f\"Number of nodes in graph: {num_nodes}\")\n",
    "print(f\"Number of edges in graph: {num_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e26651ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Largest WCC =====\n",
      "Component ID: 3\n",
      "Number of Nodes: 7066\n",
      "Number of Edges: 201472\n"
     ]
    }
   ],
   "source": [
    "# 3. Weakly Connected Components (WCC)\n",
    "\n",
    "# --- Make edges undirected ---\n",
    "undirected_edges = edges.select(\"src\", \"dst\").union(edges.select(col(\"dst\").alias(\"src\"), col(\"src\").alias(\"dst\"))).distinct()\n",
    "\n",
    "# --- Initialize each node's component as itself ---\n",
    "components = vertices.withColumn(\"component\", col(\"id\"))\n",
    "\n",
    "# --- Iteratively propagate component IDs ---\n",
    "changed = True\n",
    "iteration = 0\n",
    "while changed:\n",
    "    iteration += 1\n",
    "    # Propagate component IDs via edges\n",
    "    propagated = undirected_edges.join(components, undirected_edges.src == components.id, \"left\") \\\n",
    "                                    .select(col(\"dst\").alias(\"id\"), col(\"component\"))\n",
    "    \n",
    "    # Get the minimum component ID for each node (either itself or neighbor)\n",
    "    new_components = components.union(propagated) \\\n",
    "                                .groupBy(\"id\") \\\n",
    "                                .agg(spark_min(\"component\").alias(\"component\"))\n",
    "    \n",
    "    # Check for convergence\n",
    "    diff = components.join(new_components, \"id\") \\\n",
    "                        .filter(components.component != new_components.component)\n",
    "    \n",
    "    changed = diff.count() > 0\n",
    "    components = new_components\n",
    "\n",
    "# --- Compute WCC sizes ---\n",
    "component_sizes = components.groupBy(\"component\").agg(\n",
    "    F.countDistinct(\"id\").alias(\"num_nodes\")\n",
    ")\n",
    "\n",
    "# --- Assign component to each edge (both endpoints) ---\n",
    "edges_with_comp = undirected_edges \\\n",
    "    .join(components.withColumnRenamed(\"id\", \"src\"), on=\"src\") \\\n",
    "    .withColumnRenamed(\"component\", \"src_comp\") \\\n",
    "    .join(components.withColumnRenamed(\"id\", \"dst\").withColumnRenamed(\"component\", \"dst_comp\"), on=\"dst\")\n",
    "\n",
    "# Keep edges where both endpoints are in the same component\n",
    "edges_same_comp = edges_with_comp.filter(col(\"src_comp\") == col(\"dst_comp\"))\n",
    "\n",
    "# --- Count edges per component ---\n",
    "edges_per_comp = edges_same_comp.groupBy(\"src_comp\").agg(\n",
    "    F.count(\"*\").alias(\"num_edges\")\n",
    ").withColumnRenamed(\"src_comp\", \"component\")\n",
    "\n",
    "# --- Combine node and edge stats ---\n",
    "wcc_stats = component_sizes.join(edges_per_comp, \"component\", \"left\").fillna(0)\n",
    "\n",
    "# --- Find largest WCC ---\n",
    "largest_wcc = wcc_stats.orderBy(col(\"num_nodes\").desc()).limit(1).collect()[0]\n",
    "\n",
    "print(\"\\n===== Largest WCC =====\")\n",
    "print(f\"Component ID: {largest_wcc['component']}\")\n",
    "print(f\"Number of Nodes: {largest_wcc['num_nodes']}\")\n",
    "print(f\"Number of Edges: {largest_wcc['num_edges']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05d7791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Largest SCC =====\n",
      "  • Component ID: 1954\n",
      "  • Nodes: 1300\n",
      "  • Edges (internal): 39456\n",
      "  • Fraction of Nodes: 0.1827\n",
      "  • Fraction of Edges: 0.3805\n"
     ]
    }
   ],
   "source": [
    "# ------------------ SCC computation (Python) ------------------\n",
    "def kosaraju(vertices, edges):\n",
    "    # Collect vertices and edges to Python\n",
    "    vert_list = [int(r.id) for r in vertices.collect()]\n",
    "    edge_list = [(int(r.src), int(r.dst)) for r in edges.collect()]\n",
    "\n",
    "    # Build adjacency lists\n",
    "    out_adj = {v: [] for v in vert_list}\n",
    "    in_adj = {v: [] for v in vert_list}\n",
    "    for src, dst in edge_list:\n",
    "        out_adj[src].append(dst)\n",
    "        in_adj[dst].append(src)\n",
    "\n",
    "    # Step 1: Reverse DFS to compute finishing order\n",
    "    visited = set()\n",
    "    order = []\n",
    "\n",
    "    def dfs_rev(u):\n",
    "        stack = [(u, 0)]\n",
    "        visited.add(u)\n",
    "        while stack:\n",
    "            node, idx = stack[-1]\n",
    "            nbrs = in_adj.get(node, [])\n",
    "            if idx < len(nbrs):\n",
    "                nxt = nbrs[idx]\n",
    "                stack[-1] = (node, idx + 1)\n",
    "                if nxt not in visited:\n",
    "                    visited.add(nxt)\n",
    "                    stack.append((nxt, 0))\n",
    "            else:\n",
    "                order.append(node)\n",
    "                stack.pop()\n",
    "\n",
    "    for v in vert_list:\n",
    "        if v not in visited:\n",
    "            dfs_rev(v)\n",
    "\n",
    "    # Step 2: Forward DFS to assign components\n",
    "    comp_dict = {}\n",
    "    cid = 0\n",
    "\n",
    "    def dfs_fwd(u, cid):\n",
    "        stack = [u]\n",
    "        comp_dict[u] = cid\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            for nxt in out_adj.get(node, []):\n",
    "                if nxt not in comp_dict:\n",
    "                    comp_dict[nxt] = cid\n",
    "                    stack.append(nxt)\n",
    "\n",
    "    for v in reversed(order):\n",
    "        if v not in comp_dict:\n",
    "            cid += 1\n",
    "            dfs_fwd(v, cid)\n",
    "\n",
    "    return comp_dict, edge_list\n",
    "\n",
    "# Compute SCCs\n",
    "comp_dict, edge_list = kosaraju(vertices, edges)\n",
    "\n",
    "# ------------------ Analyze SCCs ------------------\n",
    "from collections import Counter\n",
    "\n",
    "# Count sizes of each component\n",
    "comp_sizes = Counter(comp_dict.values())\n",
    "largest_scc_id, largest_scc_nodes = max(comp_sizes.items(), key=lambda x: x[1])\n",
    "\n",
    "# Count edges internal to largest SCC\n",
    "largest_scc_set = set(v for v, cid in comp_dict.items() if cid == largest_scc_id)\n",
    "largest_scc_edges_count = sum(1 for src, dst in edge_list if src in largest_scc_set and dst in largest_scc_set)\n",
    "\n",
    "# ------------------ Print results ------------------\n",
    "print(\"\\n===== Largest SCC =====\")\n",
    "print(f\"  • Component ID: {largest_scc_id}\")\n",
    "print(f\"  • Nodes: {largest_scc_nodes}\")\n",
    "print(f\"  • Edges (internal): {largest_scc_edges_count}\")\n",
    "print(f\"  • Fraction of Nodes: {largest_scc_nodes / num_nodes:.4f}\")\n",
    "print(f\"  • Fraction of Edges: {largest_scc_edges_count / num_edges:.4f}\")\n",
    "\n",
    "# ------------------ Optional: Create DataFrame of largest SCC vertices ------------------\n",
    "largest_scc_vertices_df = spark.createDataFrame(\n",
    "    [(v,) for v in largest_scc_set],\n",
    "    schema=[\"nid\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c34d2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triangles : 608389\n",
      "Fraction of closed triplets: 0.041826\n",
      "Average clustering coefficient: 0.140898\n"
     ]
    }
   ],
   "source": [
    "# 5. Clustering Metrics & Triangles\n",
    "edges = edges.filter(F.col(\"src\") != F.col(\"dst\"))\n",
    "\n",
    "# Undirected edges (both directions)\n",
    "edges_undir = (\n",
    "    edges.select(\"src\", \"dst\")\n",
    "    .union(edges.select(F.col(\"dst\").alias(\"src\"), F.col(\"src\").alias(\"dst\")))\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "num_nodes = edges_undir.select(\"src\").union(edges_undir.select(\"dst\")).distinct().count()\n",
    "num_edges = edges.count()\n",
    "\n",
    "# ---------------- Step 1: Create adjacency lists ----------------\n",
    "adjacency = (\n",
    "    edges_undir.groupBy(\"src\")\n",
    "    .agg(F.collect_set(\"dst\").alias(\"nbrs\"))\n",
    "    .withColumnRenamed(\"src\", \"vertex\")\n",
    ")\n",
    "\n",
    "# ---------------- Step 2: Canonicalize edges (u < v) ----------------\n",
    "edges_canon = (\n",
    "    edges_undir\n",
    "    .withColumn(\"u\", F.least(F.col(\"src\"), F.col(\"dst\")))\n",
    "    .withColumn(\"v\", F.greatest(F.col(\"src\"), F.col(\"dst\")))\n",
    "    .select(\"u\", \"v\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# ---------------- Step 3: For each edge (u,v), find shared neighbors ----------------\n",
    "adj1 = adjacency.alias(\"a1\")\n",
    "adj2 = adjacency.alias(\"a2\")\n",
    "\n",
    "common_neighbors = (\n",
    "    edges_canon\n",
    "    .join(adj1, F.col(\"u\") == F.col(\"a1.vertex\"))\n",
    "    .join(adj2, F.col(\"v\") == F.col(\"a2.vertex\"))\n",
    "    .select(\n",
    "        F.col(\"u\"),\n",
    "        F.col(\"v\"),\n",
    "        F.size(F.array_intersect(F.col(\"a1.nbrs\"), F.col(\"a2.nbrs\"))).alias(\"tri_shared\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------------- Step 4: Each triangle is shared by 3 edges → divide by 3 ----------------\n",
    "total_triangles = common_neighbors.agg(F.sum(\"tri_shared\")).first()[0] / 3.0\n",
    "\n",
    "# ---------------- Step 5: Per-vertex triangle counts ----------------\n",
    "triangles_per_vertex = (\n",
    "    common_neighbors\n",
    "    .select(F.col(\"u\").alias(\"vertex\"), F.col(\"tri_shared\"))\n",
    "    .union(common_neighbors.select(F.col(\"v\").alias(\"vertex\"), F.col(\"tri_shared\")))\n",
    "    .groupBy(\"vertex\")\n",
    "    .agg(F.sum(\"tri_shared\").alias(\"triangles\"))\n",
    ")\n",
    "\n",
    "# ---------------- Step 6: Compute degree per vertex ----------------\n",
    "degree_df = (\n",
    "    edges_undir.groupBy(\"src\")\n",
    "    .agg(F.countDistinct(\"dst\").alias(\"degree\"))\n",
    "    .withColumnRenamed(\"src\", \"vertex\")\n",
    ")\n",
    "\n",
    "# ---------------- Step 7: Combine, compute clustering coefficient ----------------\n",
    "clustering_df = (\n",
    "    degree_df.join(triangles_per_vertex, \"vertex\", \"left\")\n",
    "    .fillna(0, subset=[\"triangles\"])\n",
    "    .withColumn(\"possible_triplets\", (F.col(\"degree\") * (F.col(\"degree\") - 1)) / 2)\n",
    "    .withColumn(\n",
    "        \"clustering_coeff\",\n",
    "        F.when(F.col(\"possible_triplets\") > 0,\n",
    "               F.col(\"triangles\") / F.col(\"possible_triplets\")\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------------- Step 8: Global stats ----------------\n",
    "total_triplets = clustering_df.agg(F.sum(\"possible_triplets\")).first()[0]\n",
    "fraction_closed = total_triangles / total_triplets if total_triplets > 0 else 0.0\n",
    "avg_clustering = clustering_df.agg(F.avg(\"clustering_coeff\")).first()[0]/2\n",
    "\n",
    "# ---------------- Print results ----------------\n",
    "print(f\"Total triangles : {int(total_triangles)}\")\n",
    "print(f\"Fraction of closed triplets: {fraction_closed:.6f}\")\n",
    "print(f\"Average clustering coefficient: {avg_clustering:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a7365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph Diameter: 7\n",
      "Effective Diameter (90th percentile): 4\n"
     ]
    }
   ],
   "source": [
    "# 6. Distance-based Metrics\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# ------------------ Collect graph to Python ------------------\n",
    "edge_list = [(int(r.src), int(r.dst)) for r in edges_undir.collect()]\n",
    "vert_list = [int(r.id) for r in vertices.collect()]\n",
    "\n",
    "# ------------------ Build adjacency list ------------------\n",
    "adj = {v: [] for v in vert_list}\n",
    "for src, dst in edge_list:\n",
    "    adj[src].append(dst)\n",
    "\n",
    "# ------------------ BFS function ------------------\n",
    "def bfs_shortest_paths(start, adj):\n",
    "    visited = {start: 0}  # node -> distance from start\n",
    "    q = deque([start])\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        for v in adj.get(u, []):\n",
    "            if v not in visited:\n",
    "                visited[v] = visited[u] + 1\n",
    "                q.append(v)\n",
    "    return visited  # distances from start to reachable nodes\n",
    "\n",
    "# ------------------ Compute all-pairs distances ------------------\n",
    "all_distances = []\n",
    "\n",
    "for i, node in enumerate(vert_list):\n",
    "    dist_map = bfs_shortest_paths(node, adj)\n",
    "    all_distances.extend(dist_map.values())\n",
    "\n",
    "all_distances = [d for d in all_distances if d > 0]  # ignore self-distances\n",
    "\n",
    "# ------------------ Diameter and Effective Diameter ------------------\n",
    "diameter = max(all_distances)\n",
    "effective_diameter = int(np.percentile(all_distances, 90))\n",
    "\n",
    "print(f\"\\nGraph Diameter: {diameter}\")\n",
    "print(f\"Effective Diameter (90th percentile): {effective_diameter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58600af0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
